{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf3ab2a",
   "metadata": {},
   "source": [
    "# Training a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622a434",
   "metadata": {},
   "source": [
    "* $batch:$ from a data frame is a set of samples of the whole set.\n",
    "<br>\n",
    "The **NN** is trained in parallel where one sample can be represented as \n",
    "$$\\vec{y} = \\vec{N}_{in}$$\n",
    "<br>\n",
    "Or for many samples\n",
    "$$\\vec{y} = (N_{samples}\\times\\vec{N}_{in})$$\n",
    "<br>\n",
    "* Python interprets:\n",
    "$$M = A + b \\rightarrow M_{ij} = A_{ij} + b_{j}$$\n",
    "<br>\n",
    "* Processing a batch(set of samples) for the linear function\n",
    "$$Z = dot(Y,W) + b$$\n",
    "\n",
    "where they have sizes $Z=(N_{samples}xN_{out})$, $Y=(N_{samples},N_{in})$, $W=(N_{in},N_{out})$, $b=N_{out}$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14b6f5",
   "metadata": {},
   "source": [
    "# Approximating an Arbitrary non-Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45d911",
   "metadata": {},
   "source": [
    "* Similarly to integration, a complex non-linear function may be approxiated by a coupled set of linear functions (sigmoid, reLu, etc.).\n",
    "<br>\n",
    "* **Universality of Neural Networks**: Any arbitrary (smooth) function can be approximated as well as desired by a neural network with a single hidden layer. (For sufficient number of neurons.)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47664d",
   "metadata": {},
   "source": [
    "* How to choose the values of the Weights (W) and biasses (b)? Training!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce5241",
   "metadata": {},
   "source": [
    "/home/armitage/Dropbox/Thesis master/NN_fitting.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272ae4f",
   "metadata": {},
   "source": [
    "# NN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cce29",
   "metadata": {},
   "source": [
    "* The idea is to train the NN to adjust the values of the weights and biasses to obtain the \"correct\" result. Adjust the values to approximate to the function.\n",
    "<br>\n",
    "* To train the NN is necessary to have a function that measures the difference between the output of the NN and the \"correct value\". \n",
    "* This is called the **Cost Function** and is defined as\n",
    "<br>\n",
    "$$ C(w) = \\frac{1}{2} <||F_{w}(y^{in}) - F(y^{in})||^2>$$\n",
    "<br>\n",
    "where $F_{w}(y^{in})$ is the output of the NN and $F(y^{in})$ is the correct value. The power of 2 means the average over all deviations.\n",
    "* Opimizing the NN corresponds to diminishing the Cost Function.\n",
    "* There are several methods to adjust the weights and biasses to the correct values. Two of them are: Gradient Descent and Backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c31f257",
   "metadata": {},
   "source": [
    "### 1. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2874ee8",
   "metadata": {},
   "source": [
    "* To decrease the value in the cost function, we derivate the function and take the negative value that decreases the error. So\n",
    "\n",
    "$$c'(w) \\sim -\\nabla_{w}C(w)$$\n",
    "\n",
    "* Problem: Evaluating C would mean averaging over all training samples.\n",
    "* Solution: Average over a few samples, approximate C.\n",
    "* Discrete Steps: For each step evaluate a few samples and update weights according to \n",
    "\n",
    "$$w_{j} \\rightarrow w_{j} - \\eta \\frac{\\partial \\tilde C}{\\partial w_{j}}$$\n",
    "\n",
    "$\\eta$: stepsize parameter, $\\tilde C$: Approximate version of C. In each step, different samples are taken.\n",
    "* For sufficiently small steps, sum over many steps approximates true gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f48c7b",
   "metadata": {},
   "source": [
    "* Now we evaluate the derivative of the cost function as \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{i}} = <(f(z)-F)f'(z)\\frac{\\partial z}{\\partial w_{i}}>$$\n",
    "\n",
    "where $z = \\sum_{i}^{N} w_{i}y_{i} + b$ and $\\frac{\\partial z}{\\partial w_{i}}=y_{i}$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84a9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
