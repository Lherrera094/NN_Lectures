{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf3ab2a",
   "metadata": {},
   "source": [
    "# Training a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622a434",
   "metadata": {},
   "source": [
    "* $batch:$ from a data frame, is a set of samples of the whole set.\n",
    "<br>\n",
    "The **NN** is trained in parallel where one sample can be represented as \n",
    "$$\\vec{y} = \\vec{N}_{in}$$\n",
    "<br>\n",
    "Or for many samples\n",
    "$$\\vec{y} = (N_{samples}\\times\\vec{N}_{in})$$\n",
    "<br>\n",
    "* Python interprets:\n",
    "$$M = A + b \\rightarrow M_{ij} = A_{ij} + b_{j}$$\n",
    "<br>\n",
    "* Processing a batch(set of samples) for the linear function\n",
    "$$Z = dot(Y,W) + b$$\n",
    "\n",
    "where they have sizes $Z=(N_{samples}xN_{out})$, $Y=(N_{samples},N_{in})$, $W=(N_{in},N_{out})$, $b=N_{out}$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14b6f5",
   "metadata": {},
   "source": [
    "# Approximating an Arbitrary non-Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45d911",
   "metadata": {},
   "source": [
    "* Similarly to integration, a complex non-linear function may be approxiated by a coupled set of linear functions (sigmoid, reLu, etc.).\n",
    "<br>\n",
    "* **Universality of Neural Networks**: Any arbitrary (smooth) function can be approximated as well as desired by a neural network with a single hidden layer. (For sufficient number of neurons.)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47664d",
   "metadata": {},
   "source": [
    "* How to choose the values of the Weights (W) and biasses (b)? Training!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce5241",
   "metadata": {},
   "source": [
    "/home/armitage/Dropbox/Thesis master/NN_fitting.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272ae4f",
   "metadata": {},
   "source": [
    "# NN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cce29",
   "metadata": {},
   "source": [
    "* The idea is to train the NN to adjust the values of the weights and biasses to obtain the \"correct\" result. Adjust the values to approximate to the function.\n",
    "<br>\n",
    "* To train the NN is necessary to have a function that measures the difference between the output of the NN and the \"correct value\". \n",
    "* This is called the **Cost Function** and is defined as\n",
    "<br>\n",
    "$$ C(w) = \\frac{1}{2} <||F_{w}(y^{in}) - F(y^{in})||^2>$$\n",
    "<br>\n",
    "where $F_{w}(y^{in})$ is the output of the NN and $F(y^{in})$ is the correct value. The power of 2 means the average over all deviations.\n",
    "* Opimizing the NN corresponds to diminishing the Cost Function.\n",
    "* There are several algorithms to adjust the weights and biasses to the correct values. Stchastic Gradient Descent and Backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c31f257",
   "metadata": {},
   "source": [
    "### 1. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2874ee8",
   "metadata": {},
   "source": [
    "* To decrease the value in the cost function, we derivate the function and take the negative value that decreases the error. So\n",
    "\n",
    "$$c'(w) \\sim -\\nabla_{w}C(w)$$\n",
    "\n",
    "* Problem: Evaluating C would mean averaging over all training samples.\n",
    "* Solution: Average over a few samples, approximate C.\n",
    "* Discrete Steps: For each step evaluate a few samples and update weights according to \n",
    "\n",
    "$$w_{j} \\rightarrow w_{j} - \\eta \\frac{\\partial \\tilde C}{\\partial w_{j}}$$\n",
    "\n",
    "$\\eta$: stepsize parameter, $\\tilde C$: Approximate version of C. In each step, different samples are taken.\n",
    "* For sufficiently small steps, sum over many steps approximates true gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f48c7b",
   "metadata": {},
   "source": [
    "* Now we evaluate the derivative of the cost function as \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{i}} = <(f(z)-F)f'(z)\\frac{\\partial z}{\\partial w_{i}}>$$\n",
    "\n",
    "where $z = \\sum_{i}^{N} w_{i}y_{i} + b$ and $\\frac{\\partial z}{\\partial w_{i}}=y_{i}$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38993c",
   "metadata": {},
   "source": [
    "### 2. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a52095",
   "metadata": {},
   "source": [
    "* Carefull with indices\n",
    "\n",
    "$$\\frac{\\partial C(w,y^{in})}{\\partial w_{*}} = \\sum_{j}(y_{j}^{n}-F_{j}(y^{in}))\\frac{\\partial y_{j}^{n}}{\\partial w_{*}} = \\sum_{j}(y_{j}^{n}-F_{j}(y^{in}))f'(z_{j}^{n})\\frac{\\partial z_{j}^{n}}{\\partial w_{*}}$$\n",
    "\n",
    "* Applying the chain rule repeatedly,\n",
    "\n",
    "$$\\frac{\\partial z_{j}^{n}}{\\partial w_{*}} = \\sum_{k}\\frac{\\partial z_{j}^{n}}{\\partial y_{k}^{n-1}}\\frac{\\partial y_{k}^{n-1}}{\\partial w_{*}} $$\n",
    "\n",
    "$$= \\sum_{k} w_{j,k}^{n,n-1} f'(z_{k}^{n-1})\\frac{\\partial z_{k}^{n-1}}{\\partial w_{*}}$$\n",
    "\n",
    "* The product of the first two elements at the right represent a Matrix\n",
    "\n",
    "$$M_{j,k}^{n,n-1} = w_{j,k}^{n,n-1} f'(z_{k}^{n-1})$$\n",
    "\n",
    "* By countinuing the chain rule, the problem end up being a matrix multiplication of weights and function derivatives.\n",
    "* For the bias\n",
    "\n",
    "$$\\frac{\\partial z_{j}^{n}}{\\partial b_{j}^{n}} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23267254",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c81709",
   "metadata": {},
   "source": [
    "* 1. Initialize vector from Output layer\n",
    "\n",
    "$$\\Delta_{j} = (y_{j}^{n} - f_{j}(y^{in}))f'(z_{j}^{n})$$\n",
    "\n",
    "* 2. For each layer, saves the outcome\n",
    "\n",
    "$$\\frac{\\partial C(w, y^{in})}{\\partial w_{*}} = \\Delta_{j} \\frac{\\partial z_{j}^{n}}{\\partial w_{*}}$$\n",
    "\n",
    "* 3. Multiply vector by matrix\n",
    "\n",
    "$$\\Delta_{k}^{new} = \\sum_{j} \\Delta_{j} M_{jk}^{n,n-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701402df",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ea4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390373ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_f_df(z):\n",
    "    \n",
    "    f = 1/(1+exp(-z))\n",
    "    df = exp(-z)*(f**2)\n",
    "    \n",
    "    return f, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f77d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(y,w,b):           #Calculate values in next step\n",
    "    z = dot(y,w) + b               #Save result for next layer    \n",
    "    \n",
    "    return net_f_df(z)             #Apply nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61e93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_net(y_in):\n",
    "    global Weights, Biasses, NumLayers\n",
    "    global y_layer, df_layer\n",
    "    \n",
    "    y = y_in\n",
    "    y_layer[0] = y\n",
    "    \n",
    "    for j in range(NumLayers):\n",
    "        #j=0 first layer after input layer\n",
    "        y, df = forward_step(y, Weights[j], Biasses[j])\n",
    "        df_layer[j] = df\n",
    "        y_layer[j+1] = y\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e91be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation\n",
    "def backward_step(delta,w,df):\n",
    "    \n",
    "    # delta al layer N of batchsize x layersize(N)\n",
    "    # w = [layersize(N-1) x layersize(N)] matrix\n",
    "    # df = df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "    \n",
    "    return dot(delta, transpose(W))*df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e92d36c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(y_target):\n",
    "    # the result will be the \"dw_layer\" matrices with\n",
    "    # with the derivatives of the cost function with respect to the weights and biasses\n",
    "    \n",
    "    global y_layer, df_layer, Weights, Biasses, NumLayers\n",
    "    global dw_layer, db_layer\n",
    "    global batchsize\n",
    "    \n",
    "    delta = (y_layer - y_targe)*df_layer\n",
    "    dw_layer[-1] = dot( transpose(y_layer[-2]), delta)/ batchsize\n",
    "    db_layer[-1] = delta.sum(0)/batchsize\n",
    "    \n",
    "    for j in range(NumLayers-1):\n",
    "        delta = backward_step(delta, Weights[-1-j], df_layer[-2,-j])\n",
    "        dw_layer[-2-j] = dot( transpose(y_layer[-3-j]), delta)/batchsize\n",
    "        db_layer[-2-j] = delta.sum(0)/batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0762cfd",
   "metadata": {},
   "source": [
    "## Problem Specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0babd",
   "metadata": {},
   "source": [
    "* 1. Choose Network Layout: number of layers, number of neurons in each layer, type of nonlinear functions, specialized structure of weights. (**Hyperparameters**)\n",
    "* 2. Generate training (and validation and test) samples: usually load from big databases or produced by software.\n",
    "* 3. Monitor/Optimize training progress (choose learning rate and batchsize).\n",
    "\n",
    "* **Hyperparameters**: specific value of the NN that are choose by the programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05a688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
